{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "031d7ee0-f4de-4201-85fd-d32761e036cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the task value from the previous tasks (bronze & silver)\n",
    "bronze_output = dbutils.jobs.taskValues.get(taskKey=\"Bronze\", key=\"bronze_output\")\n",
    "silver_output = dbutils.jobs.taskValues.get(taskKey=\"Silver\", key=\"silver_output\")\n",
    "\n",
    "# Access individual variables\n",
    "start_date = bronze_output.get(\"start_date\",\"\")\n",
    "#bronze_adls = bronze_output.get(\"bronze_output_path\",\"\")\n",
    "silver_adls = bronze_output.get(\"silver_adls\",\"\")\n",
    "gold_adls = bronze_output.get(\"gold_adls\",\"\")\n",
    "\n",
    "print(f\"Start Date: {start_date}, Gold ADLS:{gold_adls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbd52e8-1d27-4015-a319-dd6bed03441e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "#Ensure the below library is installed on your cluster\n",
    "import reverse_geocoder as rg\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aecbc40-e596-4868-9d7e-d624cd4e87ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We are reading the data for just that date.\n",
    "df = spark.read.parquet(silver_output).filter(col('time') > start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd18952a-2cca-4831-a4e6-767f920a2290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.limit(500) \n",
    "# added this tep to speed up processing as we read the data\n",
    "#The problem is caused by the Python UDF (reverse_geocoder) being a bottleneck due to its non-paralle nature and high computational cost per task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca1819d-24e5-4e82-a49b-018a84523c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creating the UDF that performs the reverse geocoding\n",
    "def get_county_code(lat, lon):\n",
    "    '''\n",
    "    Retrieve the country code for a given latitude and logitude.\n",
    "    \n",
    "    Parameters:\n",
    "    lat (float or str): Latitude of the Location.\n",
    "    lon (float or str): Longitude of the Location.\n",
    "\n",
    "    Returns:\n",
    "    str: Country code of the location, retriveded using the reverse geocoder API\n",
    "\n",
    "    Example:\n",
    "    >>> get_county_code(48.8588443, 2.2943506)\n",
    "    'FR'\n",
    "    '''\n",
    "    try:\n",
    "        coordinates = (float(lat), float(lon))\n",
    "        result = rg.search(coordinates)[0].get('cc')\n",
    "        print(f\"Processed Coordinates: {coordinates} -> {result}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing coordinates: {lat}, {lon} -> {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "258ca7db-a4b6-478c-ad08-4b836c49aa28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Register the above udf so that they can be used on the spark dataframes\n",
    "get_country_code_udf = udf(get_county_code, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8598a437-ec21-4157-90d3-4a09a493201e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "get_county_code(48.8588443, 2.2943506)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7231577a-976c-404a-956c-41366d4aaa69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adding country code and city attributes\n",
    "df_with_location = \\\n",
    "                df.\\\n",
    "                    withColumn(\"country_code\", get_country_code_udf(col(\"latitude\"), col(\"longitude\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3ae08ea-4640-4319-b61d-e345ecba50bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adding signifncance classification\n",
    "df_with_location_sig_class = \\\n",
    "                df_with_location.\\\n",
    "                    withColumn('sig_class',\n",
    "                               when(col(\"sig\")< 100, \"Low\").\\\n",
    "                                when((col(\"sig\")>=100) & (col(\"sig\") < 500), \"Moderate\").\\\n",
    "                                otherwise(\"High\")\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "316cf82a-839e-4e4c-93c2-f348ca804584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_location_sig_class.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "280ca21b-3b5f-46c8-8e7f-2281399a989a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Save the transformed DataFrame to the Gold Container\n",
    "gold_output_path = f\"{gold_adls}earthquakes_events_gold/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a074d5a4-182c-4a4e-b82c-90edbff96059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Append DataFrame to gold container in Parquet format\n",
    "df_with_location_sig_class.write.mode(\"append\").parquet(gold_output_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
